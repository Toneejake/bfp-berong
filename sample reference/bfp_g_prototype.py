# -*- coding: utf-8 -*-
"""bfp g prototype

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I4Lml0pqEsPoH1Fo48i_oxjmbkHzazwk
"""

# Cell 1: Installations (CORRECTED)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q
!pip install datasets Pillow matplotlib scikit-learn -q

# Cell 2: Imports and Configuration (MODIFIED)
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms
from datasets import load_dataset
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm # MODIFIED: Import tqdm for progress bars

# Configuration dictionary
config = {
    "dataset_name": "zimhe/pseudo-floor-plan-12k",
    "image_size": 256,
    "batch_size": 8,   # MODIFIED: Set to the working batch size
    "learning_rate": 1e-4,
    "epochs": 15,
    "val_split": 0.1
}

# Cell 3: The Custom Dataset Class
class FloorPlanDataset(Dataset):
    def __init__(self, hf_dataset, transform=None):
        self.hf_dataset = hf_dataset
        self.transform = transform

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        item = self.hf_dataset[idx]

        # 'plans' is the input image, 'walls' is the target mask
        input_image = item['plans'].convert("RGB")
        target_mask = item['walls'].convert("L") # Grayscale

        if self.transform:
            # Seed to ensure same random transformation for both image and mask
            seed = np.random.randint(2147483647)

            torch.manual_seed(seed)
            input_image = self.transform(input_image)

            torch.manual_seed(seed)
            target_mask = self.transform(target_mask)

        return input_image, target_mask

# Cell 4: Load and Prepare the Dataset (MODIFIED FOR T4 GPU)

# Define transformations for the images
transform = transforms.Compose([
    transforms.Resize((config['image_size'], config['image_size'])),
    transforms.ToTensor(),
])

# Load the dataset from Hugging Face
full_dataset_hf = load_dataset(config['dataset_name'], split='train')
full_dataset = FloorPlanDataset(full_dataset_hf, transform=transform)

# Split into training and validation sets
val_size = int(len(full_dataset) * config['val_split'])
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

# Create DataLoaders with optimizations
# MODIFIED: Added num_workers and pin_memory for faster data loading
train_loader = DataLoader(
    train_dataset,
    batch_size=config['batch_size'],
    shuffle=True,
    num_workers=2,
    pin_memory=True
)
val_loader = DataLoader(
    val_dataset,
    batch_size=config['batch_size'],
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

print(f"Dataset loaded successfully.")
print(f"Training set size: {len(train_dataset)}")
print(f"Validation set size: {len(val_dataset)}")

# Cell 5: The U-Net Model Architecture (MODIFIED)
class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()

        def double_conv(in_channels, out_channels):
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                nn.ReLU(inplace=True)
            )

        self.dconv_down1 = double_conv(3, 64)
        self.dconv_down2 = double_conv(64, 128)
        self.dconv_down3 = double_conv(128, 256)
        self.dconv_down4 = double_conv(256, 512)

        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.dconv_up3 = double_conv(256 + 512, 256)
        self.dconv_up2 = double_conv(128 + 256, 128)
        self.dconv_up1 = double_conv(128 + 64, 64)

        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)
        # MODIFIED: Removed self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        conv1 = self.dconv_down1(x)
        x = self.maxpool(conv1)

        conv2 = self.dconv_down2(x)
        x = self.maxpool(conv2)

        conv3 = self.dconv_down3(x)
        x = self.maxpool(conv3)

        x = self.dconv_down4(x)

        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        x = torch.cat([x, conv3], dim=1)
        x = self.dconv_up3(x)

        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        x = torch.cat([x, conv2], dim=1)
        x = self.dconv_up2(x)

        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        x = torch.cat([x, conv1], dim=1)
        x = self.dconv_up1(x)

        out = self.conv_last(x)
        return out # MODIFIED: Removed the final sigmoid() call

# Cell 6: The Training Loop (MODIFIED WITH TQDM)

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize model, loss function, and optimizer
model = UNet().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])
scaler = torch.cuda.amp.GradScaler()

# --- TRAINING STARTS HERE ---
for epoch in range(config['epochs']):
    model.train()
    train_loss = 0.0

    # MODIFIED: Wrap train_loader with tqdm for a progress bar
    train_iterator = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['epochs']} [Training]")

    for inputs, masks in train_iterator:
        inputs, masks = inputs.to(device), masks.to(device)

        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            outputs = model(inputs)
            loss = criterion(outputs, masks)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        current_loss = loss.item()
        train_loss += current_loss * inputs.size(0)

        # MODIFIED: Update the progress bar with the current loss
        train_iterator.set_postfix(loss=f"{current_loss:.6f}")

    # --- VALIDATION ---
    model.eval()
    val_loss = 0.0

    # MODIFIED: Wrap val_loader with tqdm for a progress bar
    val_iterator = tqdm(val_loader, desc=f"Epoch {epoch+1}/{config['epochs']} [Validation]")

    with torch.no_grad():
        for inputs, masks in val_iterator:
            inputs, masks = inputs.to(device), masks.to(device)
            with torch.cuda.amp.autocast():
                outputs = model(inputs)
                loss = criterion(outputs, masks)
            val_loss += loss.item() * inputs.size(0)

    # Calculate and print average losses for the epoch
    train_loss = train_loss / len(train_dataset)
    val_loss = val_loss / len(val_dataset)

    print(f"\nEpoch {epoch+1} Summary: "
          f"Avg. Train loss: {train_loss:.6f}, "
          f"Avg. Validation loss: {val_loss:.6f}\n")

print("\n--- Training Complete ---")

# Cell 7: Save the Trained Model
MODEL_PATH = "unet_floorplan_model.pth"
torch.save(model.state_dict(), MODEL_PATH)
print(f"Model saved to {MODEL_PATH}")

# Cell 8: Test and Visualize the Results (MODIFIED)

# Load the model from the saved file
model.load_state_dict(torch.load(MODEL_PATH))
model.eval()

# Get a random batch from the validation loader
inputs, masks = next(iter(val_loader))
inputs, masks = inputs.to(device), masks.to(device)

# Get model prediction
with torch.no_grad():
    outputs = model(inputs)

# Move tensors to CPU for plotting and select the first image in the batch
inputs = inputs.cpu()
masks = masks.cpu()
outputs = outputs.cpu()
idx_to_show = 0

# Prepare images for display
original_image = transforms.ToPILImage()(inputs[idx_to_show])
true_mask = transforms.ToPILImage()(masks[idx_to_show])
# MODIFIED: Apply torch.sigmoid() to the raw model output (logits)
predicted_mask = transforms.ToPILImage()((torch.sigmoid(outputs[idx_to_show]) > 0.5).float())

# Plotting
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(original_image)
axes[0].set_title("Original Floor Plan")
axes[0].axis('off')

axes[1].imshow(true_mask, cmap='gray')
axes[1].set_title("Ground Truth Walls")
axes[1].axis('off')

axes[2].imshow(predicted_mask, cmap='gray')
axes[2].set_title("AI Predicted Walls")
axes[2].axis('off')

plt.show()

# Cell 9: The Complete Inference Pipeline
import torch
from PIL import Image
from torchvision import transforms
import numpy as np

def create_grid_from_image(model, image_path, image_size, device):
    """
    Takes a trained U-Net model and an image file path,
    and returns a binary numpy grid (1=wall, 0=free).
    """
    # 1. Load and prepare the model
    model.to(device)
    model.eval()

    # 2. Define the same transformations used during training
    transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
    ])

    # 3. Load and transform the input image
    try:
        image = Image.open(image_path).convert("RGB")
        input_tensor = transform(image).unsqueeze(0).to(device)
    except Exception as e:
        print(f"Error opening or processing image: {e}")
        return None

    # 4. Get the model's prediction
    with torch.no_grad():
        output = model(input_tensor)

    # 5. Process the output into a binary grid
    # Apply sigmoid because our model outputs logits
    output_probs = torch.sigmoid(output)
    # Use a 0.5 threshold to decide wall vs. free space
    binary_mask = (output_probs > 0.5).float()

    # Squeeze the tensor to remove batch and channel dimensions, move to CPU
    grid_tensor = binary_mask.squeeze().cpu()

    # 6. Convert to NumPy array
    grid_numpy = grid_tensor.numpy()

    return grid_numpy

print("Inference function 'create_grid_from_image' is defined.")

# Cell 10: Test the Pipeline with a New Image
from google.colab import files
import matplotlib.pyplot as plt

# --- Step 1: Upload a new floor plan image ---
print("Please upload a new floor plan image to test the pipeline.")
uploaded = files.upload()

if not uploaded:
    print("\nNo file uploaded. Please run the cell again.")
else:
    # --- Step 2: Process the uploaded image ---
    test_image_path = list(uploaded.keys())[0]

    # Re-use variables from our previous setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    image_size = config['image_size'] # From Cell 2

    # The model is already loaded, but for a standalone script you'd load it here:
    # model.load_state_dict(torch.load(MODEL_PATH))

    print("\nProcessing image...")
    final_grid = create_grid_from_image(model, test_image_path, image_size, device)

    # --- Step 3: Display the result ---
    if final_grid is not None:
        print("\nSuccessfully created the simulation grid!")

        plt.figure(figsize=(8, 8))
        plt.imshow(final_grid, cmap='gray')
        plt.title("Generated Binary Grid (1=Wall, 0=Free Space)")
        plt.xticks([])
        plt.yticks([])
        plt.show()

# Cell 11: Core Simulation Logic (A* Pathfinding, Fire, and Person Classes)
import heapq
import numpy as np

# --- 1. A* Pathfinding Algorithm ---
def a_star_search(grid, start, goal, fire_map=None):
    """Finds the shortest path on a grid using A* search."""
    # Heuristic function (Manhattan distance)
    def heuristic(a, b):
        return abs(a[0] - b[0]) + abs(a[1] - b[1])

    # A* algorithm implementation
    neighbors = [(0, 1), (0, -1), (1, 0), (-1, 0)] # 4-directional movement
    close_set = set()
    came_from = {}
    gscore = {start: 0}
    fscore = {start: heuristic(start, goal)}
    oheap = []

    heapq.heappush(oheap, (fscore[start], start))

    while oheap:
        current = heapq.heappop(oheap)[1]

        if current == goal:
            data = []
            while current in came_from:
                data.append(current)
                current = came_from[current]
            data.reverse()
            return data

        close_set.add(current)
        for i, j in neighbors:
            neighbor = current[0] + i, current[1] + j
            tentative_g_score = gscore[current] + 1

            # Check if neighbor is valid
            if 0 <= neighbor[0] < grid.shape[1] and 0 <= neighbor[1] < grid.shape[0]:
                # Check for walls or fire
                if grid[neighbor[1]][neighbor[0]] == 1:
                    continue
                if fire_map is not None and fire_map[neighbor[1]][neighbor[0]] == 1:
                    continue
            else:
                # Neighbor is out of bounds
                continue

            if neighbor in close_set and tentative_g_score >= gscore.get(neighbor, 0):
                continue

            if tentative_g_score < gscore.get(neighbor, 0) or neighbor not in [i[1] for i in oheap]:
                came_from[neighbor] = current
                gscore[neighbor] = tentative_g_score
                fscore[neighbor] = tentative_g_score + heuristic(neighbor, goal)
                heapq.heappush(oheap, (fscore[neighbor], neighbor))

    return [] # Return empty path if no path is found

# --- 2. Fire Simulator Class ---
class FireSimulator:
    def __init__(self, grid, spread_probability=0.25):
        self.grid = grid
        self.spread_probability = spread_probability
        self.fire_map = np.zeros_like(grid)
        self.directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]

    def start_fire(self, ignition_points):
        for y, x in ignition_points:
            if 0 <= y < self.fire_map.shape[0] and 0 <= x < self.fire_map.shape[1]:
                self.fire_map[y, x] = 1

    def step(self):
        new_fire_map = self.fire_map.copy()
        rows, cols = self.fire_map.shape

        # Find all currently burning cells
        burning_cells = np.argwhere(self.fire_map == 1)

        for r, c in burning_cells:
            for dr, dc in self.directions:
                nr, nc = r + dr, c + dc

                if 0 <= nr < rows and 0 <= nc < cols:
                    # Check if neighbor is unburned and not a wall
                    if self.fire_map[nr, nc] == 0 and self.grid[nr, nc] == 0:
                        if np.random.rand() < self.spread_probability:
                            new_fire_map[nr, nc] = 1

        self.fire_map = new_fire_map

    def reset(self):
        self.fire_map = np.zeros_like(self.grid)

# --- 3. Person Agent Class ---
class Person:
    def __init__(self, position):
        self.pos = position
        self.path = []
        self.status = 'evacuating' # evacuating, escaped, burned

    def compute_path(self, grid, goal, fire_map):
        start_pos = (int(self.pos[0]), int(self.pos[1]))
        goal_pos = (int(goal[0]), int(goal[1]))
        self.path = a_star_search(grid, start_pos, goal_pos, fire_map)

    def move(self):
        if self.path:
            # Move to the next point in the path and remove it from the list
            self.pos = self.path.pop(0)

    def check_status(self, fire_map, exits):
        pos_int = (int(self.pos[0]), int(self.pos[1]))
        # Check if burned
        if fire_map[pos_int[1]][pos_int[0]] == 1:
            self.status = 'burned'
        # Check if escaped (close enough to an exit)
        for ex in exits:
            if np.linalg.norm(np.array(self.pos) - np.array(ex)) < 5: # 5 pixel radius for escape
                self.status = 'escaped'

print("Core simulation classes (A*, FireSimulator, Person) are defined.")

# Cell 11.5: Install OpenCV
!pip install opencv-python-headless -q

# Cell 12: The Gymnasium Environment (OPTIMIZED)
import gymnasium as gym
from gymnasium import spaces
import cv2 # Use OpenCV for much faster resizing

class EvacuationEnv(gym.Env):
    def __init__(self, grid, num_agents=3, max_steps=500):
        super(EvacuationEnv, self).__init__()
        self.grid = grid
        self.num_agents = num_agents
        self.max_steps = max_steps
        self.exits = self._find_exits()
        if not self.exits:
            raise ValueError("No exits found in the provided grid.")

        self.fire_sim = FireSimulator(self.grid)
        self.agents = []

        self.action_space = spaces.Discrete(len(self.exits))

        # MODIFIED: Define observation shape components clearly
        self.fire_obs_shape = 64 * 64
        self.agent_obs_shape = self.num_agents * 2
        obs_shape = self.fire_obs_shape + self.agent_obs_shape + 1
        self.observation_space = spaces.Box(low=0, high=1, shape=(obs_shape,), dtype=np.float32)

    def _find_exits(self):
        # This logic is fine as it only runs once during initialization
        rows, cols = self.grid.shape
        exits = []
        for x in range(cols):
            if self.grid[1, x] == 0: exits.append((x, 1))
            if self.grid[rows-2, x] == 0: exits.append((x, rows-2))
        for y in range(rows):
            if self.grid[y, 1] == 0: exits.append((y, 1)) # Typo fixed here
            if self.grid[y, cols-2] == 0: exits.append((y, cols-2)) # Typo fixed here

        if not exits: return []
        filtered_exits = [exits[0]]
        for ex in exits:
            if all(np.linalg.norm(np.array(ex) - np.array(f_ex)) > 20 for f_ex in filtered_exits):
                filtered_exits.append(ex)
        return filtered_exits

    def _get_observation(self):
        # MODIFIED: Replaced slow PyTorch transforms with fast OpenCV resizing
        fire_map_resized = cv2.resize(self.fire_sim.fire_map.astype(np.float32), (64, 64), interpolation=cv2.INTER_AREA)
        fire_obs = fire_map_resized.flatten()

        agent_pos = np.array([agent.pos for agent in self.agents]).flatten()
        # Normalize agent positions
        agent_obs = agent_pos / np.array([self.grid.shape[1], self.grid.shape[0]] * self.num_agents)

        time_obs = np.array([self.current_step / self.max_steps])

        return np.concatenate([fire_obs, agent_obs, time_obs]).astype(np.float32)

    def reset(self, seed=None):
        super().reset(seed=seed)
        self.current_step = 0
        self.fire_sim.reset()
        center_y, center_x = self.grid.shape[0] // 2, self.grid.shape[1] // 2
        self.fire_sim.start_fire([(center_y, center_x)])

        self.agents = []
        while len(self.agents) < self.num_agents:
            y, x = np.random.randint(0, self.grid.shape[0]), np.random.randint(0, self.grid.shape[1])
            if self.grid[y, x] == 0 and self.fire_sim.fire_map[y,x] == 0:
                self.agents.append(Person(position=(x, y)))

        return self._get_observation(), {}

    def step(self, action):
        self.current_step += 1
        self.fire_sim.step()

        target_exit = self.exits[action]
        reward = -0.01

        for agent in self.agents:
            if agent.status == 'evacuating':
                if not agent.path or self.current_step % 10 == 0:
                    agent.compute_path(self.grid, target_exit, self.fire_sim.fire_map)

                agent.move()
                agent.check_status(self.fire_sim.fire_map, self.exits)

                if agent.status == 'escaped':
                    reward += 10
                elif agent.status == 'burned':
                    reward -= 10

        terminated = all(agent.status != 'evacuating' for agent in self.agents)
        truncated = self.current_step >= self.max_steps
        observation = self._get_observation()

        return observation, reward, terminated, truncated, {}

print("Optimized Gymnasium Environment 'EvacuationEnv' is defined.")

# Cell 13: Test Drive the Environment with a Random Agent
import time
from IPython.display import clear_output
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# --- Use the 'final_grid' from our Perception AI (Cell 10) ---
if 'final_grid' not in locals():
    print("Error: 'final_grid' not found. Please re-run Cell 10 to generate it.")
else:
    # --- Initialize the Environment ---
    env = EvacuationEnv(grid=final_grid, num_agents=5)
    obs, info = env.reset()

    terminated = False
    truncated = False

    # --- Lists to store history for visualization ---
    frame_history = []

    # --- The Simulation Loop ---
    while not terminated and not truncated:
        # Instead of a smart AI, we pick a random action
        action = env.action_space.sample()

        # Take a step in the environment
        obs, reward, terminated, truncated, info = env.step(action)

        # --- Store data for this frame ---
        frame_data = {
            'fire_map': env.fire_sim.fire_map.copy(),
            'agents': [(agent.pos, agent.status) for agent in env.agents]
        }
        frame_history.append(frame_data)

    # --- Final Status Report ---
    escaped_count = sum(1 for agent in env.agents if agent.status == 'escaped')
    burned_count = sum(1 for agent in env.agents if agent.status == 'burned')
    evacuating_count = sum(1 for agent in env.agents if agent.status == 'evacuating')

    print("--- Simulation Test Drive Complete ---")
    print(f"Total Steps: {env.current_step}")
    print(f"Agents Escaped: {escaped_count}")
    print(f"Agents Burned: {burned_count}")
    print(f"Agents Still Evacuating: {evacuating_count}")

    # --- Simple Visualization of the Final State ---
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(env.grid, cmap='gray')

    final_state = frame_history[-1]
    ax.imshow(final_state['fire_map'], cmap='Reds', alpha=0.6)

    for pos, status in final_state['agents']:
        color = 'blue'
        if status == 'escaped': color = 'green'
        elif status == 'burned': color = 'black'
        dot = patches.Circle((pos[0], pos[1]), radius=3, color=color)
        ax.add_patch(dot)

    for ex in env.exits:
      exit_circle = patches.Circle(ex, radius=5, color='lime', alpha=0.7)
      ax.add_patch(exit_circle)

    ax.set_title("Final State of the Random Simulation")
    plt.show()

# Cell 14: Install Stable Baselines3
# We need to install a specific version of gymnasium that is compatible with stable-baselines3
!pip install stable-baselines3[extra] gymnasium -q

# Cell 15: Train the AI Commander (PPO Agent)
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

# --- Use the 'final_grid' from our Perception AI (Cell 10) ---
if 'final_grid' not in locals():
    print("Error: 'final_grid' not found. Please re-run Cell 10 to generate it.")
else:
    # --- 1. Create the Vectorized Environment ---
    # We create a function that returns an instance of our environment
    env_fn = lambda: EvacuationEnv(grid=final_grid, num_agents=5, max_steps=500)

    # make_vec_env is a helper that prepares environments for SB3
    vec_env = make_vec_env(env_fn, n_envs=1)

    # --- 2. Define and Train the PPO Model ---
    # "MlpPolicy" means the AI uses a standard neural network for decisions
    model_ppo = PPO("MlpPolicy", vec_env, verbose=1)

    # This is where the training happens. We'll train for 100,000 steps.
    print("\n--- Starting AI Commander Training ---")
    model_ppo.learn(total_timesteps=100000)
    print("\n--- AI Commander Training Complete ---")

    # --- 3. Save the Trained Model ---
    PPO_MODEL_PATH = "ppo_evacuation_commander.zip"
    model_ppo.save(PPO_MODEL_PATH)
    print(f"\nAI Commander model saved to {PPO_MODEL_PATH}")

# Run this in a new cell after interrupting the training
PPO_PARTIAL_MODEL_PATH = "ppo_commander_24k_steps.zip"
model_ppo.save(PPO_PARTIAL_MODEL_PATH)

print(f"Successfully saved partially trained model to {PPO_PARTIAL_MODEL_PATH}")

# Cell 16: The Grand Finale - Full Pipeline and Animated Visualization
from google.colab import files
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import time

# --- Part 1: Run the Full Pipeline ---

# 1a. Upload the floor plan
print("--- SafeScape Final Simulation ---")
print("Please upload a floor plan for the final simulation.")
uploaded = files.upload()

if not uploaded:
    print("\nNo file uploaded. Please run the cell again to generate the final report.")
else:
    test_image_path = list(uploaded.keys())[0]

    # 1b. Process image with Perception AI (Module 1)
    print("Step 1: Analyzing floor plan with Perception AI...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    image_size = config['image_size']
    # Ensure the U-Net model is still in memory and in eval mode
    model.eval()
    simulation_grid = create_grid_from_image(model, test_image_path, image_size, device)
    print("Floor plan analyzed successfully.")

    # 1c. Setup Environment and Load AI Commander (Module 2 & 3)
    print("Step 2: Preparing simulation environment and loading AI Commander...")
    env = EvacuationEnv(grid=simulation_grid, num_agents=5, max_steps=500)
    PPO_MODEL_PATH = "ppo_commander_24k_steps.zip"
    ppo_model = PPO.load(PPO_MODEL_PATH)
    print("Environment and AI Commander are ready.")

    # 1d. Run the Intelligent Simulation & Record History
    print("Step 3: Running intelligent evacuation simulation...")
    obs, _ = env.reset()
    terminated, truncated = False, False
    history = []

    while not terminated and not truncated:
        action, _ = ppo_model.predict(obs, deterministic=True)
        obs, _, terminated, truncated, _ = env.step(action)

        # Record the state of the world at this step
        frame_data = {
            'fire_map': env.fire_sim.fire_map.copy(),
            'agents': [(agent.pos, agent.status) for agent in env.agents],
            'step': env.current_step
        }
        history.append(frame_data)
    print("Simulation complete.")

    # --- Part 2: Generate the Final Outputs ---

    # 2a. Generate the Animation
    print("Step 4: Generating final animation...")
    fig, ax = plt.subplots(figsize=(10, 10))
    original_image_for_bg = Image.open(test_image_path).resize((image_size, image_size))

    def animate(frame_index):
        ax.clear()
        frame_data = history[frame_index]

        # Background is the user's floor plan
        ax.imshow(original_image_for_bg, extent=[0, image_size, image_size, 0])

        # Overlay fire
        ax.imshow(frame_data['fire_map'], cmap='Reds', alpha=0.5, extent=[0, image_size, image_size, 0])

        # Draw agents
        for pos, status in frame_data['agents']:
            color = 'blue'
            if status == 'escaped': color = 'lime'
            elif status == 'burned': color = 'black'
            dot = patches.Circle((pos[0], pos[1]), radius=3, color=color)
            ax.add_patch(dot)

        # Draw exits
        for ex in env.exits:
            exit_circle = patches.Circle(ex, radius=5, color='green', alpha=0.9, fill=False, linewidth=3)
            ax.add_patch(exit_circle)

        ax.set_title(f"AI-Controlled Evacuation | Step: {frame_data['step']}", fontsize=16)
        ax.set_xticks([])
        ax.set_yticks([])

    anim = FuncAnimation(fig, animate, frames=len(history), interval=100)
    plt.close() # Prevents static plot from displaying

    # Display the animation in Colab
    display(HTML(anim.to_jshtml()))

    # 2b. Generate the Risk Assessment Dashboard
    final_state = history[-1]
    escaped = sum(1 for _, status in final_state['agents'] if status == 'escaped')
    burned = sum(1 for _, status in final_state['agents'] if status == 'burned')
    total_agents = len(final_state['agents'])

    print("\n\n" + "="*40)
    print("|      SafeScape Risk Assessment       |")
    print("="*40)
    print(f"| Floor Plan: {test_image_path}")
    print(f"| Simulation Time: {final_state['step']} steps")
    print("-" * 40)
    print("| --- Evacuation Outcome ---")
    print(f"| Total Agents:       {total_agents}")
    print(f"| Agents Escaped:     {escaped} ({escaped/total_agents:.1%})")
    print(f"| Agents Burned:      {burned} ({burned/total_agents:.1%})")
    print("-" * 40)

    risk_level = "LOW"
    if burned / total_agents > 0.5:
      risk_level = "CRITICAL"
    elif burned / total_agents > 0.2:
      risk_level = "HIGH"
    elif burned / total_agents > 0:
      risk_level = "MEDIUM"

    print(f"| --- Safety Insights ---")
    print(f"| Calculated Risk Level: {risk_level}")
    print("="*40)